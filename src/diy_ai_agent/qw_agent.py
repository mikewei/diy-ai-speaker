import urllib.parse
import json5
from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
from qwen_agent.utils.output_beautify import typewriter_print
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from typing import List
from datetime import datetime


@register_tool('get_datetime')
class GetDatetime(BaseTool):
    description = 'Get the current date, time and weekday.'
    parameters = []

    def call(self, _params: str, **kwargs) -> str:
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S %A')


@register_tool('my_image_gen')
class MyImageGen(BaseTool):
    # The `description` tells the agent the functionality of this tool.
    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{
        'name': 'prompt',
        'type': 'string',
        'description': 'Detailed description of the desired image content, in English',
        'required': True
    }]

    def call(self, params: str, **kwargs) -> str:
        # `params` are the arguments generated by the LLM agent.
        prompt = json5.loads(params)['prompt']  # type: ignore
        prompt = urllib.parse.quote(prompt)
        return json5.dumps(
            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},
            ensure_ascii=False)


# Configure the LLM you are using.
llm_cfg = {
    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:
    'model': 'qwen3-14b-diy',
    'model_server': 'http://localhost:8000/v1',  # base_url, also known as api_base
    'api_key': 'EMPTY',

    # (Optional) LLM hyperparameters for generation:
    'generate_cfg': {
        'thought_in_content': True,
        'temperature': 0.6,
        'top_p': 0.95,
        'top_k': 20
    }
}

# Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.
# system_instruction = '''After receiving the user's request, you should:
# - first draw an image and obtain the image url,
# - then run code `request.get(image_url)` to download the image,
# - and finally select an image operation from the given document to process the image.
# Please show the image using `plt.show()`.'''
system_instruction = '''
You are a helpful assistant that can answer questions and help with tasks. 

# Global Behavior Guidelines

- Do not call functions (<tool_call>) between <think> and </think> tags, call functions only after </think> tag.
- I am a parrot, and my name is 小金刚.
- If user's question is simple, please think fast and short.

# Current Task Instructions (Optional)
'''

tools = [
    'get_datetime',
    'my_image_gen',
    'code_interpreter',  # `code_interpreter` is a built-in tool for executing code.
    {
        'mcpServers': {
            "whiteboard-mcp": {
                "command": "npx",
                "args": [
                    "mcp-remote",
                    "http://192.168.0.110:5000/mcp/sse",
                    "--allow-http"
                ]
            }
        }
    }
]

files = [
    '/home/mikewei/data/datasets/mikewei/basic_facts_3.md'
]

bot = Assistant(llm=llm_cfg,
                system_message=system_instruction,
                function_list=tools,  # type: ignore
                files=files)


if __name__ == "__main__":
    messages = []  # This stores the chat history.
    while True:
        # For example, enter the query "draw a dog and rotate it 90 degrees".
        query = input('prompt >>> ')
        # Append the user query to the chat history.
        messages.append({'role': 'user', 'content': query})
        response = []
        response_plain_text = ''
        print('---------------------')
        for response in bot.run(messages=messages):
            # Streaming output.
            response_plain_text = typewriter_print(response, response_plain_text)  # type: ignore
            # print(f'----------------{response}----------------\n')
        # Append the bot responses to the chat history.
        messages.extend(response)
        print('\n==================================================')
        print(f'len of response: {len(response)}')
        for msg in response:
            print(msg)


# 创建FastAPI应用
app = FastAPI(title="Smart Agent API")

# 添加CORS中间件
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]

class ChatResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[dict]

def extract_prefix_thinking(msg_content: str) -> tuple[str, str]:
    """
    Extract and remove the prefix <think> and </think> from the message content.
    """
    reasoning = ""
    msg_content = msg_content.lstrip()
    if msg_content.startswith('<think>'):
        msg_content = msg_content[7:]
    think_end = msg_content.find('</think>')
    # todo: when </think> tag missing, how to return?
    if think_end != -1:
        reasoning = msg_content[:think_end].lstrip()
        msg_content = msg_content[think_end + 8:].lstrip()
    return reasoning, msg_content

@app.post("/v1/chat/completions", response_model=ChatResponse)
async def chat_completion(request: ChatRequest):
    try:
        # todo: optimize the log format
        print(f"get request: {request}")    
        messages = [msg.model_dump() for msg in request.messages]
        response_content = ""
        response_reasoning = ""
        
        for response in bot.run(messages=messages):  # type: ignore
            pass
        print(f"get response: {response}")    
        for msg in response:
            if msg['role'] == 'assistant' and msg['content'] != '':
                reasoning, content = extract_prefix_thinking(msg['content'])
                response_reasoning += reasoning
                response_content += content
        print(f"return response reasoning: {response_reasoning} content: {response_content}")    
        
        return ChatResponse(
            id=f"chatcmpl-{int(datetime.now().timestamp())}",
            created=int(datetime.now().timestamp()),
            model=llm_cfg['model'],
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_content,
                    "reasoning": response_reasoning
                },
                "finish_reason": "stop"
            }],
            # usage={
            #     "prompt_tokens": 0,  # 这里可以添加实际的 token 计数
            #     "completion_tokens": 0,
            #     "total_tokens": 0
            # }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/v1/models")
async def get_models():
    return {
        "data": [
            {
                "id": "qwen3-14b-diy",
                "object": "model",
            }
        ]
    }